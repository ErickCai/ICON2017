{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICON 2017 Multivoxel pattern analysis workshop\n",
    "Welcome to this workshop!\n",
    "\n",
    "###### 1. Goals\n",
    "In this workshop, we cover the basics of applying multivoxel pattern analysis (MVPA) to analyze (functional) MRI data. After this tutorial, you will understand:\n",
    "\n",
    "- The differences between within-subject and between-subject pattern analysis\n",
    "- Extracting patterns from fMRI data\n",
    "- Methods for feature extraction and selection\n",
    "- Model selection and evaluation\n",
    "\n",
    "In short, this tutorial will show how to build your own complete, cross-validated pipeline to perform MVPA.\n",
    "\n",
    "###### 2. Pre-requisites\n",
    "For this tutorial, we assume that you are familiar with analyzing (f)MRI data and want to know how you would go about implementing a machine-learning (“decoding”) analysis in Python. As such, it’s definitely helpful if you have some knowledge about Python’s syntax, but it’s not strictly necessary. Also, we assume participants are relatively unfamiliar with machine learning (ML) concepts and their implementation; if you’re a ML guru, this is probably not the right workshop for you.\n",
    "\n",
    "###### 3. Scope and content\n",
    "There is actually no single \"multivoxel pattern analysis\", rather, MVPA is a bucket-term referring to several kinds of analyses. As this tutorial cannot cover all such analyses, we limit the scope to *within-subject* designs and *classification* analyses. We hope to cover these grounds sufficiently well for you to be able to explore other kinds of analyses own your own. Inspired by this workshop and want more? Definitely try to implement between-subject designs and regression!\n",
    "\n",
    "The tutorial is split in two parts. In the first part, we cover the basics of experimental designs, and illustrate how to extract patterns from raw fMRI data in a way that MVPA can be applied. In the second part, MVPA proper is discussed: feature extraction and selection, and model selection and evaluation. We challenge you to optimize your pipelines to maximize your decoding accuracy scores.\n",
    "\n",
    "Have fun and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. In case you have not yet downloaded the data used in this workshop\n",
    "Foei! Do it now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Experimental design and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Designs for pattern analysis\n",
    "There are many ways in which you can categorize different types of pattern analyses, but one of the most basic categorizations is in terms of whether analyses are **within-subject** or **between-subject**. The major distinction revolves around whether you want to investigate an (experimental) factor that varies or is manipulated within subjects or that varies across subjects (i.e. individual differences or experimental between-subject designs).\n",
    "\n",
    "Importantly, these types of analyses differ in what is regarded as an *instance of a pattern*:\n",
    "- in within-subject analyses, *each instance of your feature-of-interest represents one pattern* (e.g., each single trial);\n",
    "- in between-subject analyses, *each subject represents one pattern*.\n",
    "\n",
    "This distinction is important, because the choice of type of pattern analysis has major consequences for the design of your experiment, and the methods available for pattern extraction. **Throughout this tutorial, we focus on within-subject analyses.** We will extract *patterns* of $\\beta$-estimates by fitting a hemodynamic response function (HRF) per trial using a general linear model (GLM). There are other methods available of pattern extraction; but this method is known to yeild the most stable pattern estimates and is therefore used most often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Within-subject designs\n",
    "Often, \"trials\" (i.e. instances of your feature-of-interest) in within-subject designs are modelled as separate regressors in a first-level analysis. In other words, you model each trial as a separate (*single!*) HRF-response. Below, we included an image of a single-trial design (of the hypothetical faces vs. houses experiment) as created in FSL:<img src=\"single_trial_design.png\" alt=\"Drawing\" heigth=\"100\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each trial gets it's own regressor (hence the name single-trial design). Then, as depicted below the design matrix, a contrast-against-baseline is created for each regressor (trial). After you run a first-level analysis using this design, you'll have whole-brain maps containing statistics values ($\\beta$-values, *t*-values, or *z*-values) for each trial that represent the trial's estimated (whole-brain) pattern. Usually, *t*-values or *z*-values are used instead of $\\beta$-values.  \n",
    "\n",
    "**Importantly, this design thus specifies that each \"trial\" represents a sample with its own pattern (voxels).**\n",
    "\n",
    "Before you go on, make sure you understand this image! This image represents basically all you need to understand about single-trial designs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement your own single-trial design\n",
    "To further illustrate how single-trial designs work, let's create your own single-trial matrix corresponding to a (real) working memory experiment. In this experiment, one condition (\"ACTIVE\") required subjects to remember a configuration of bars, and after a retention period had to respond whether one of the bars has changed in the test-image or not. In the other condition (\"PASSIVE\") they just watched a blank screen and had to respond with a random answer. The experiment is depicted schematically below:\n",
    "\n",
    "![test](WM_example.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, subjects performed 40 trials, of which 32 were of the \"ACTIVE\" condition and 8 were of the \"PASSIVE\" condition. In the following section, we'll generate a single-trial design that aims to estimate the pattern for each trial.\n",
    "\n",
    "Below, we'll load the trial onset times (and trial durations and conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we need to import some Python packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onset Duration Condition\n",
      "[[  8   6   1]\n",
      " [ 20   6   1]\n",
      " [ 26   6   1]\n",
      " [ 36   6   0]\n",
      " [ 42   6   1]\n",
      " [ 52   6   1]\n",
      " [ 60   6   1]\n",
      " [ 66   6   1]\n",
      " [ 72   6   1]\n",
      " [ 78   6   1]\n",
      " [ 86   6   1]\n",
      " [ 92   6   1]\n",
      " [100   6   1]\n",
      " [106   6   0]\n",
      " [112   6   1]\n",
      " [118   6   1]\n",
      " [126   6   0]\n",
      " [134   6   1]\n",
      " [140   6   1]\n",
      " [150   6   1]\n",
      " [156   6   0]\n",
      " [162   6   0]\n",
      " [172   6   1]\n",
      " [180   6   1]\n",
      " [188   6   1]\n",
      " [194   6   1]\n",
      " [204   6   0]\n",
      " [210   6   1]\n",
      " [216   6   1]\n",
      " [222   6   1]\n",
      " [228   6   1]\n",
      " [238   6   1]\n",
      " [244   6   1]\n",
      " [252   6   1]\n",
      " [258   6   1]\n",
      " [264   6   1]\n",
      " [272   6   0]\n",
      " [280   6   0]\n",
      " [296   6   1]\n",
      " [302   6   1]]\n"
     ]
    }
   ],
   "source": [
    "# The onset times are loaded as a Numpy array with three columns: \n",
    "# onset times (in seconds) (column 1), durations (column 2), and conditions (column 3).\n",
    "# N.B.: condition 0 = passive, condition 1 = active\n",
    "onsets = np.loadtxt('onsets.csv').astype(int)\n",
    "print(\"Onset Duration Condition\")\n",
    "print(onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Practice your numpy-skills! How would you calculate how many active-trials and how many passive-trials there were? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 32 active trials and 8 passive trials\n"
     ]
    }
   ],
   "source": [
    "# Example answer\n",
    "n_active_trials = np.sum(onsets[:,2])\n",
    "n_passive_trials = np.sum(onsets[:,2]==0)\n",
    "print('There were %d active trials and %d passive trials' %(n_active_trials, n_passive_trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the onsets (and duration) are here defined in seconds (not TRs). Let's assume that the fMRI-run has a TR of 2. Now, we can convert (very easily!) the onsets/durations-in-seconds to onsets/durations-in-TRs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Convert the onsets and durations from seconds to TRs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4   3   1]\n",
      " [ 10   3   1]\n",
      " [ 13   3   1]\n",
      " [ 18   3   0]\n",
      " [ 21   3   1]\n",
      " [ 26   3   1]\n",
      " [ 30   3   1]\n",
      " [ 33   3   1]\n",
      " [ 36   3   1]\n",
      " [ 39   3   1]\n",
      " [ 43   3   1]\n",
      " [ 46   3   1]\n",
      " [ 50   3   1]\n",
      " [ 53   3   0]\n",
      " [ 56   3   1]\n",
      " [ 59   3   1]\n",
      " [ 63   3   0]\n",
      " [ 67   3   1]\n",
      " [ 70   3   1]\n",
      " [ 75   3   1]\n",
      " [ 78   3   0]\n",
      " [ 81   3   0]\n",
      " [ 86   3   1]\n",
      " [ 90   3   1]\n",
      " [ 94   3   1]\n",
      " [ 97   3   1]\n",
      " [102   3   0]\n",
      " [105   3   1]\n",
      " [108   3   1]\n",
      " [111   3   1]\n",
      " [114   3   1]\n",
      " [119   3   1]\n",
      " [122   3   1]\n",
      " [126   3   1]\n",
      " [129   3   1]\n",
      " [132   3   1]\n",
      " [136   3   0]\n",
      " [140   3   0]\n",
      " [148   3   1]\n",
      " [151   3   1]]\n"
     ]
    }
   ],
   "source": [
    "# Example answer\n",
    "onsets_tr = onsets.copy()\n",
    "onsets_tr[:,0:2] = onsets_tr[:,0:2]/2\n",
    "print(onsets_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the first-level analysis, for each regressor (trial) we need to create a regressor of zeros and ones, in which the ones represent the moments in which the particular trial was presented. Note that, if a stimulus lasted 6 seconds (i.e. 3 TRs), make sure that your regressor also models your event for this duration! \n",
    "\n",
    "So, for example, if you have a (hypothetical) run with a total duration of 15 TRs, and you show a stimulus at TR=3 for the duration of 3 TRs (i.e. 6 seconds), then you'd code your regressor as:\n",
    "\n",
    "`[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, we initialized a stimulus vector (`stim_vec`) of shape=[162, 40], i.e. timepoints x trials (this run was 162 TRs long), with zeros. Each of the 40 rows represents one trial. Loop over the colums of the `stim_vec` matrix and fill the times at onset till the onset + 2 TRs with ones. Remember, the first index in Python is zero (not 1!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the stim_vec variable with ones at the indices of the onsets per trial!\n",
    "stim_vec = np.zeros((162, 40))\n",
    "\n",
    "# Example answer\n",
    "for column in range(stim_vec.shape[1]):\n",
    "    stim_vec[onsets_tr[column,0]:(onsets_tr[column,0]+onsets_tr[column,1]),column] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to convolve an HRF with the stimulus-vectors and we'll have a complete single-trial design! Don't worry, we do this for you. We'll also plot it to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJYAAAJkCAYAAABDHatbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10908ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import double_gamma\n",
    "\n",
    "hrf = double_gamma(range(162))\n",
    "\n",
    "# List comprehension (fancy for-loop) + stack results back to a matrix\n",
    "X = np.vstack([np.convolve(hrf, stim_vec[:, i], 'full')[:162] for i in range(40)]).T\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for plot in range(40):\n",
    "    plt.subplot(1, 40, plot+1)\n",
    "    plt.plot(X[:, plot], range(X.shape[0])[::-1])\n",
    "    plt.axis('off')\n",
    "    plt.text(-2, 170, 'Active' if onsets[plot, 2] else 'Passive', rotation=45)\n",
    "\n",
    "plt.text(-60, -10, 'Regressors (trials)', ha='center', fontsize=20)\n",
    "plt.text(-145, 80, 'Time (TR)', va='center', rotation='vertical', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this design to extract patterns of $\\beta$-values for each regressor. But [it has been shown](http://www.sciencedirect.com/science/article/pii/S1053811910007834) that converting $\\beta$-values to *t*-values often creates more stable and robust patterns. $\\beta$-values can be normalized to *t*-values by defining a contrast-against-baseline that is subsequently used in the formula for the *t*-value. \n",
    "\n",
    "**ik snap deze vraag niet helemaal: hoe heeft het omrekenen van beta's naar t-waardes invloed op je contrast matrix?**\n",
    "\n",
    "Suppose I want to convert the patterns of $\\beta$-values *for each trial* in the design above to *t*-values, what would my contrast-matrix look like? (Hint: check out the first image of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: create a design matrix in which each row represents the contrast-against-baseline-vector of a single trial of the within-subject design above. (Hint: check out the `numpy.eye()` function)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code\n",
    "np.eye(onsets.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you know how to create a \"single-trial design\" for pattern analyses! As a short summary:\n",
    "- Model each \"trial\" (or more generally, \"instance\") as a separate HRF-convolved regressor;\n",
    "- Additionally define a contrast-matrix in which each trial is contrasted against baseline in order to create *t*-value/*z*-value patterns;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: We ran the first-level analysis of the single-trial working-memory design outlined above for fifteen subjects using FSL. You can find the results in the directory: `data/{subject_number}/wm.feat`. Pick one of the subjects, and check out the `stats` subdirectory. You will see forty tstat-niftis, each containing a whole-brain pattern of *t*-stats of a single trial. These contain the data we will use in this tutorial for MVPA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, you now know what a single-trial (within-subject) design looks like and what it produces (i.e. single-trial pattern estimates in the form of whole-brain $\\beta$/*t*-stat/*z*-stat maps). Before we go on to between-subject designs, we will show what a pattern of a single trial looks like in fslview. This will hopefully give you some more \"intuition\" on what is meant with a single trial pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**If you have FSL installed on your computer, you can do this yourself!** open up a new terminal or terminal-tab and start fslview (by typing `fslview` in the terminal). Now, click \"File\" > \"Open\", navigate to `data/pi0031/wm.feat/stats` and select `tstat1.nii.gz`. Now, to visualize the patterns somewhat more intuitively, set in the fslview header \"Min\" to 0 and \"Max\" to 5. Then, click the little circle with the blue \"i\": <img src=\"viz.png\">\n",
    "<br>\n",
    "Under the \"lookup table options\", select the \"Red-Yellow\" colormap. What you visualize here are all the voxels that activate during this particular trial. However, a pattern is not necessarily only the voxels that *activate*, but also those that *deactivate*. A pattern is *any* estimated response of the brain during an instance of a sample (here: a trial). Therefore, let's also visualize the deactivating voxels. To do so, click \"File\" > \"Add\" and select the `tstat1.nii.gz` image *again*, but now set \"Min\" to 0 and \"Max\" to -5. For this file, select the colormap \"Blue-Lightblue\". \n",
    "\n",
    "What you should see now is something like this: <img src=\"screenshot_within.png\">\n",
    "\n",
    "<br>\n",
    "What we visualized here is an example of a within-subject pattern of a particular trial (\"sample\"), in which the activated voxels (relative to baseline) are colored red/yellow, and the deactivated voxels (relative to baseline) are colored blue/lightblue. Make sure you understand how this image represents the pattern of a single sample.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data representation\n",
    "In pattern analyses, there is a specific way to 'store' and represent brain patterns: as 2D matrices of shape N-samples \\* N-voxels. **Important**: often (and confusingly), people refer to voxels as (brain) 'features' in pattern analyses. So in articles people often refer to samples-by-features matrices!\n",
    "\n",
    "Anyway, this is what such a matrix looks like: \n",
    "![](data_representation.png)\n",
    "\n",
    "Each row thus represents the voxel pattern of a sample (or: \"instance\")!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the originally 3D voxel patterns (e.g. whole-brain patterns of *t*-values) are flattened (also called \"vectorized\" or \"raveled\") such that we can stack all patterns vertically into a 2D matrix. There are two reasons why pattern analyses need this 2D format and thus discard spatial information about the voxel patterns:\n",
    " \n",
    "1. There are very few analyses that take spatial information into account in the first place (so by flattening we get rid of the spatial information); \n",
    "2. Most algorithms used \"under the hood\" by pattern analyses rely heavily on matrix algebra (which operate on 2D matrices by definition).\n",
    "\n",
    "Anyway, let's look at an example. We're going to work with the working-memory data (as outline in the beginning of the notebook). Suppose we want to investigate whether we can predict whether a trial is passive or active (factor: working memory load) from (whole-brain) voxel patterns. Consequently, this is a **within-subject design**. As such, we model each trial separately by fitting a single-trial design matrix to obtain patterns of *t*-values per trial (similar to the plot just before Section **1.3**). The results are in the directory: `data/pi0031/wm.feat`. Check out the directory (again) and especially the stats-folder. You should see a bunch of nifti-files which contain 3D voxel patterns with \"tstat\" values (FSL also produces pes [betas], copes, varcopes - we removed these to minimize data size of this tutorial).\n",
    "\n",
    "For this analysis, we're going to use patterns of *t*-stats (as is generally recommended over $\\beta$s).<br>\n",
    "As you can see, there are 40 nifti-files with *t*-stats; these refer to the 40 trials in the experiment (32 active trials, 8 passive trials)! Given that we need to adhere to the data representation as outlined above, we are in the following situation:\n",
    "\n",
    "**What we have**: 40 (3D) nifti-files<br>\n",
    "**What we need**: one 2D numpy array of shape 40 x {whatever amount of voxels there are in those niftis}\n",
    "\n",
    "Alright, time to learn some Python gems that help us load in and transform those patterns into a usable 2D numpy matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 tips & tricks to load and transform (nifti-)files\n",
    "As a first thing, we need to find all the paths to the *t*-stat nifti-files. Python has a nifty (pun intended) tool called \"`glob`\" which can find files/directories on disk using [wildcards](https://en.wikipedia.org/wiki/Wildcard_character). It is usually imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glob`, in Python, is a function that takes a path (as a string) with one or more wildcard characters (such as the `*`) and searches for files/directories on disk that match that. For example, let's try to find all the png-images in the current directory using glob (these are the images that I used inside this notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_representation.png', 'fslview.png', 'obj_class_diff.png', 'pipelinesX.png', 'screenshot_within.png', 'single_trial_design.png', 'sklearn_transformers.png', 'viz.png', 'within_subject_example.png', 'WM_example.png']\n"
     ]
    }
   ],
   "source": [
    "my_search_string = '*.png'\n",
    "png_files = glob(my_search_string)\n",
    "print(png_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns a list with all the files/directories that matched the search-string. Note that you can also search files outside of the current directory. To do so, we can simply specify the relative or absolute path to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Now you have the skills to actually \"glob\" all the *t*-stats yourself! Use glob to find all the paths to the t-stats and store the results (a list with 40 strings) in a variable called `tstat_paths`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example answer\n",
    "tstat_paths = glob('../data/pi0031/wm.feat/stats/*.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: `glob` returns unsorted paths (so in seemingly random order). It's better if we sort the paths before loading them in, so the order of the paths is more intuitive (the first file is tstat1, the seconds tstat2, etc.). Python has a builtin function `sorted()`, which takes a list and sorts it alphabetically. The problem, here, is that if we'd use that - i.e. `sorted(tstat_paths)` - it will actually sort the files as: tstat1, tstat10, tstat11, etc. See for yourself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/pi0031/wm.feat/stats/tstat1.nii.gz', '../data/pi0031/wm.feat/stats/tstat10.nii.gz', '../data/pi0031/wm.feat/stats/tstat11.nii.gz', '../data/pi0031/wm.feat/stats/tstat12.nii.gz', '../data/pi0031/wm.feat/stats/tstat13.nii.gz', '../data/pi0031/wm.feat/stats/tstat14.nii.gz', '../data/pi0031/wm.feat/stats/tstat15.nii.gz', '../data/pi0031/wm.feat/stats/tstat16.nii.gz', '../data/pi0031/wm.feat/stats/tstat17.nii.gz', '../data/pi0031/wm.feat/stats/tstat18.nii.gz', '../data/pi0031/wm.feat/stats/tstat19.nii.gz', '../data/pi0031/wm.feat/stats/tstat2.nii.gz', '../data/pi0031/wm.feat/stats/tstat20.nii.gz', '../data/pi0031/wm.feat/stats/tstat21.nii.gz', '../data/pi0031/wm.feat/stats/tstat22.nii.gz', '../data/pi0031/wm.feat/stats/tstat23.nii.gz', '../data/pi0031/wm.feat/stats/tstat24.nii.gz', '../data/pi0031/wm.feat/stats/tstat25.nii.gz', '../data/pi0031/wm.feat/stats/tstat26.nii.gz', '../data/pi0031/wm.feat/stats/tstat27.nii.gz', '../data/pi0031/wm.feat/stats/tstat28.nii.gz', '../data/pi0031/wm.feat/stats/tstat29.nii.gz', '../data/pi0031/wm.feat/stats/tstat3.nii.gz', '../data/pi0031/wm.feat/stats/tstat30.nii.gz', '../data/pi0031/wm.feat/stats/tstat31.nii.gz', '../data/pi0031/wm.feat/stats/tstat32.nii.gz', '../data/pi0031/wm.feat/stats/tstat33.nii.gz', '../data/pi0031/wm.feat/stats/tstat34.nii.gz', '../data/pi0031/wm.feat/stats/tstat35.nii.gz', '../data/pi0031/wm.feat/stats/tstat36.nii.gz', '../data/pi0031/wm.feat/stats/tstat37.nii.gz', '../data/pi0031/wm.feat/stats/tstat38.nii.gz', '../data/pi0031/wm.feat/stats/tstat39.nii.gz', '../data/pi0031/wm.feat/stats/tstat4.nii.gz', '../data/pi0031/wm.feat/stats/tstat40.nii.gz', '../data/pi0031/wm.feat/stats/tstat5.nii.gz', '../data/pi0031/wm.feat/stats/tstat6.nii.gz', '../data/pi0031/wm.feat/stats/tstat7.nii.gz', '../data/pi0031/wm.feat/stats/tstat8.nii.gz', '../data/pi0031/wm.feat/stats/tstat9.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(tstat_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this issue, we wrote a little function (`sort_nifti_paths()`) that sorts the paths correctly. (If you're interested in how it works, check out the functions.py file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's fix it\n",
    "from functions import sort_nifti_paths\n",
    "tstat_paths = sort_nifti_paths(tstat_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the paths to all the nifti-files of a single subject. To load a nifti-file into a numpy-array in Python, we can use the nibabel package. This package has two useful methods you can use to load your data: `load` and `get_data`. You need to use them consecutively to load the *t*-values. For example, to load the whole-brain pattern of *t*-values of the first trial of subject pi0031:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "data = nib.load('../data/pi0031/wm.feat/stats/tstat1.nii.gz').get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: in the code block below, write a loop that loads in the tstat nifti-files one by one (using nibabel) and store them in the already preallocated array \"X\". Note that \"X\" is a 2D matrix (samples-by-features), but each tstat-file contains a 3D array, so you need to \"flatten\" the 3D array to a single vector: use e.g. the numpy function \"flatten()\" or \"ravel()\". \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voxel_dims = (80, 80, 37) # The data is in EPI-space\n",
    "X = np.zeros((len(tstat_paths), np.prod(voxel_dims)))\n",
    "\n",
    "# Example answer\n",
    "for trial, tstat_path in enumerate(tstat_paths):\n",
    "    data = nib.load(tstat_path).get_data()\n",
    "    data = data.ravel()\n",
    "    X[trial,:] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can we check if X is correct here? Would be a good check before continuing to part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include skbold MVP here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You've reached a milestone!\n",
    "Starting with raw fMRI data, you now have patterns in the right format (a 2D-matrix of N-samples x N-features). Now, let's start talking about multivariate pattern analyses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Multivoxel pattern analysis\n",
    "In Part 1, we discussed within-subject single-trial designs, and showed how to load single-trial data in the right format to perform MVPA. It's now time to actually implement pattern analyses!\n",
    "\n",
    "We'll be looking at the implementation of concepts like K-fold cross-validation, feature-selection/extraction, model fitting/prediction, permutation testing, and feature visualization. To do so, we'll use the [`scikit-learn`](http://scikit-learn.org) machine learning package in Python (the de-facto and most-used ML package in Python).\n",
    "\n",
    "Remember that, very generally, MVPA is all about inferring a feature-of-interest (dependent variable) using patterns of fMRI data. Let's call the feature-of-interest **y**, and the patterns of fMRI data **X**. This inference from X to y is called *decoding*: we want to *decode* a dependent variable y from X.\n",
    "\n",
    "In our case, let's continue with the data from the working memory experiment used in Part 1. Remember that, in this experiment, participants performed two types of trials: \"ACTIVE\" trials, in which they had to remember the orientation of eight bars, and \"PASSIVE\" trials in which they did not do anything (note that the screen shortly flickered in the \"PASSIVE\" trials in order to activate the visual cortex). Let's try to decode trial type from the patterns of *t*-values you creates in Part 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Adding the dependent variable y\n",
    "In Section 1.3, you ended with a nice 2D-matrix of N-samples x N-features. This 2D-matrix contains all whole-brain patterns of *t*-values for all trials: this is your **X**. However, this leaves out a crucial part of the data: the actual feature-of-interest, *trial type*, your **y**.\n",
    "\n",
    "While there is kind of a generic way to load in voxel patterns, there is usually not a single way to load in your dependent variable (y), because the exact factor that represents `y` dependent on your exact research question (and also depends how you have stored this data on disk). \n",
    "\n",
    "In within-subject single-trial designs, trial type or condition is often the dependent variable. The dependent variable can thus be extracted from your design. In fact, we already loaded the dependent variable previously, in the `onsets` variable (see Section 1.2). The third column contains the trial types, where 1 is an \"ACTIVE\" trial and 0 a \"PASSIVE\" trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8   6   1]\n",
      " [ 20   6   1]\n",
      " [ 26   6   1]\n",
      " [ 36   6   0]\n",
      " [ 42   6   1]\n",
      " [ 52   6   1]\n",
      " [ 60   6   1]\n",
      " [ 66   6   1]\n",
      " [ 72   6   1]\n",
      " [ 78   6   1]\n",
      " [ 86   6   1]\n",
      " [ 92   6   1]\n",
      " [100   6   1]\n",
      " [106   6   0]\n",
      " [112   6   1]\n",
      " [118   6   1]\n",
      " [126   6   0]\n",
      " [134   6   1]\n",
      " [140   6   1]\n",
      " [150   6   1]\n",
      " [156   6   0]\n",
      " [162   6   0]\n",
      " [172   6   1]\n",
      " [180   6   1]\n",
      " [188   6   1]\n",
      " [194   6   1]\n",
      " [204   6   0]\n",
      " [210   6   1]\n",
      " [216   6   1]\n",
      " [222   6   1]\n",
      " [228   6   1]\n",
      " [238   6   1]\n",
      " [244   6   1]\n",
      " [252   6   1]\n",
      " [258   6   1]\n",
      " [264   6   1]\n",
      " [272   6   0]\n",
      " [280   6   0]\n",
      " [296   6   1]\n",
      " [302   6   1]]\n"
     ]
    }
   ],
   "source": [
    "print(onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Extract vector *y* from `onsets`, which only contains the trial types (i.e., y = [1, 1, 1, 0, ..., 0, 1, 1]) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "# Dit moet straks anders\n",
    "\n",
    "import pandas as pd\n",
    "onsets_df = pd.DataFrame(onsets, columns=['onsets', 'dur', 'cond'])\n",
    "onsets_df = onsets_df.sort_values(['cond', 'onsets'], ascending=[False, True])\n",
    "y = onsets_df['cond'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What's next ...\n",
    "Awesome, we now have everything we need (X and y). This took a while, but you'll soon see it was worth the effort.\n",
    "\n",
    "In the rest of the tutorial, you'll finally learn how to actually implement decoding pipelines. Typical pipelines consist of the following elements:\n",
    "\n",
    "* Data partitioning\n",
    "* Feature selection/extraction\n",
    "* Model fitting (on train-set)\n",
    "* Model cross-validation (on test-set)\n",
    "* Calculate model performance\n",
    "* Statistical analyses of performance\n",
    "* Optional: feature visualization \n",
    "\n",
    "In the rest of the tutorial, we'll discuss these topics in a slightly different order. First, we'll discuss how you can fit and cross-validate scikit-learn models, and while we're at it, how to calculate model performance (here: accuracy). Subsequently, you'll learn how to embed these concepts in fully cross-validated K-fold data partitioning schemes. Finally, we'll extend our pipelines with feature selection methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Model fitting & cross-validation\n",
    "First, we'll show you how to use scikit-learn models. In fact, the most useful functionality of scikit-learn is probably that they made fitting and cross-validating models (or, in scikit-learn lingo: estimators) trivially easy. \n",
    "\n",
    "Note that in the upcoming example, we will fit the model on *all* our samples. In practice, this is something you would *never* do: you always need to cross-validate your model to a new, independent sample (i.e., independent with respect to noise: the noise in your *training* sample should not be correlated with the noise in your *test* set).\n",
    "\n",
    "Anyway, let's import a scikit-learn model: the (linear) support vector classifier ([SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), which is one of the most-often used models in fMRI pattern analyses. In scikit-learn, this model is part of the (suprising...) `svm` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scikit-learn is always imported as 'sklearn'\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most of scikit-learn's functionality, SVC is a *class*. Let's initialize an SVC-object! One important argument that this object needs is the [\"kernel\"](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html) you want the model to have. Basically, the kernel determines how to treat/process your features: linearly, or non-linearly (such as the `kernel='rbf'` or `kernel='poly'` options). Most often a linear kernel is the best option (as non-linear kernels seem to overfit very quickly).\n",
    "\n",
    "To initialize an SVC-object with a linear kernel, just do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf = CLassiFier\n",
    "clf = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we initialized an SVC-object with a linear kernel. Now, you need to do two thing to get the prediction for each sample (i.e. whether they're predicted as 0 or 1): fit, using the method `fit(X, y)`, and predict the class (i.e. 0 or 1) for each class using the method `predict(X)`. Basically, in the `fit` method, the parameters of the model (i.e. $\\beta$) are estimated. Then, in the `predict` method, the estimated parameters are used to generate a prediction for each sample (i.e. 0 or 1). \n",
    "\n",
    "Let's first look at the `fit` method. As you can see, the `fit(X, y)` method with two parameters: X (a samples-by-features matrix) and y (a vector of length n-samples). Let's do that for our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted SVC...\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X, y)\n",
    "print('Fitted SVC...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling the fit() method, the clf-object contains an attribute `coef_` that represent the model's parameters ('coefficients' in scikit-learn lingo, i.e. $\\beta$). Let's check that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of coefficients: (1, 236800)\n"
     ]
    }
   ],
   "source": [
    "coefs = clf.coef_\n",
    "print(\"Shape of coefficients: %r\" % (coefs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, just like we expected: the `coef_` attribute is exactly the same size as the number of voxels in our X-matrix (i.e. 80\\*80\\*37). \n",
    "\n",
    "Anyway, usually, you don't do anything directly with the weights (perhaps only if you want to do anything with feature visualization): scikit-learn handles that for you. What you *do* want, is an actual prediction ($\\hat{y}$) for our samples!\n",
    "\n",
    "To get this, simply call the `predict(X)` method of the model, which returns the predictions as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions for my samples are:\n",
      " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "y_hat = clf.predict(X)\n",
    "print(\"The predictions for my samples are:\\n %r\" % y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical next step is to assess how good the model was in predicting the class of the samples. A straightforward metric\\* to summarize performance is *accuracy* which can be defined as: \n",
    "\n",
    "\\begin{align}\n",
    "accuracy = \\frac{number\\ of\\ correct\\ predictions}{number\\ of\\ predictions}\n",
    "\\end{align}\n",
    "\n",
    "---------------\n",
    "\\* There are waaaay [more metrics to summarize model performance](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) for classification models, such as precision, recall, F1-score, and ROC-AUC. These metrics are more appropriate than accuracy when you have *imbalanced classes*, i.e. more samples in one class (e.g. negative images) than in another class (e.g. positive images). Usually, however, experimental designs in fMRI pattern analyses are (more of less) balanced, so often you can just use accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Can you calculate the accuracy of the above model? Hint 1: you need to compare the true labels (i.e. mvp.y) with the predicted labels (i.e. y_hat). Hint 2: if you do arithmetic with boolean values (i.e. `True` and `False`), `True` is interpreted as 1 and `False` is interpreted as 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Implement your to-do here!\n",
    "accuracy = np.sum(y_hat==y) / len(y_hat)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done the ToDo above correctly, you should have found out that the accuracy was 1.0 - a perfect score! \"Awesome! Nature Neuroscience material!\", you might think. But, as is almost always the case: if it's too good to be true, it probably *is* indeed too good to be true.\n",
    "\n",
    "So, what is the issue here? Well, we didn't cross-validate the model! We fitted it on all the samples in the mvp-object and predicted the *same* samples, which leads to optimistic estimate of model performance. Such optimistic estimates in uncross-validated models are especially likely when there are many more features (here: voxels) than samples (here: subjects). In other words, we are probably *overfitting* the model here.\n",
    "\n",
    "Thus, let's check what happens if we actually cross-validate the model. To do so, we need to partition the data into a train- and test-set. For this example, we'll use a simple hold-out scheme, in which we'll reserve half of the data for the test-set (we'll discuss more intricate cross-validation schemes such as K-fold in the next section).\n",
    "\n",
    "Below, we index `X` and `y` such that the train-set will contain all odd-numbered samples and the test-set will contain all even-number samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (20, 236800)\n",
      "Shape y_train: (20,)\n",
      "Shape X_test: (20, 236800)\n",
      "Shape y_test: (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X[1::2]\n",
    "print(\"Shape X_train: %r\" % (X_train.shape,))\n",
    "\n",
    "y_train = y[1::2]\n",
    "print(\"Shape y_train: %r\" % (y_train.shape,))\n",
    "\n",
    "X_test = X[0::2]\n",
    "print(\"Shape X_test: %r\" % (X_test.shape,))\n",
    "\n",
    "y_test = y[0::2]\n",
    "print(\"Shape y_test: %r\" % (y_test.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: now it's up to you to actually implement the *cross-validated* equivalent of the fit/predict procedure we showed you before. So, fit your model on `X_train` and `y_train` and then predict `X_test`. Calculate the cross-validated accuracy by comparing the predictions with `y_test`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "# Implement your ToDo here\n",
    "\n",
    "# Example answer\n",
    "mod = clf.fit(X=X_train, y=y_train)\n",
    "y_test_hat = clf.predict(X=X_test)\n",
    "accuracy = np.sum(y_test_hat==y_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! You actually performed your first proper decoding analysis in the ToDo above! There are however a couple of things we can do to improve the efficiency and results. One thing we can do is to use the data more efficiently in cross-validation. In the previous example, we split the data into two sets and fit the model on one (the train-set) and cross-validated to the other (the test-set). There is actually a way to \"re-used\" the data by using K-fold cross-validation, in which data is iteratively partitioned in train- and test-sets. This is explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data partitioning: K-fold cross-validation\n",
    "There are two principled ways of cross-validation: \n",
    "\n",
    "* Hold-out cross-validation;\n",
    "* K-fold cross-validation;\n",
    "\n",
    "In the previous section we implemented a form of hold-out cross-validation, which can be seen as a kind of \"one-shot cross-validation\". In fMRI data-sets, however, we usually have few samples (simply because MRI-data is expensive to acquire!). Because K-fold cross-validation allows you to reuse the data, it is much more common than hold-out cross-validation in MRI studies.\n",
    "\n",
    "Now, we can finally use some of scikit-learn's functionality. We are going to use the [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) object from scikit-learn's model_selection module. Click the highlighted link above and read through the manual to see how it works.\n",
    "\n",
    "Importantly, if you're dealing with a classification analysis, always use *Stratified*KFold (instead of the regular [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)), because this version makes sure that each fold contains the same proportion of the different classes (here: 0 and 1). Anyway, enough talking. Let's initialize a StratifiedKFold object with 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikit-learn is imported as 'sklearn'\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# They call folds 'splits' in scikit-learn\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have a StratifiedKFold object now, but not yet any indices for our folds (i.e. indices to split X and y into different samples). To do that, we need to call the `split(X, y)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = skf.split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we created the variable `folds` which is officially a [generator](https://wiki.python.org/moin/Generators) object, but just think of it as a type of list (with indices) which is specialized for looping over it. Each entry in `folds` is a tuple with two elements: an array with train-indices and an array with test-indices. Let's demonstrate that\\*:\n",
    "\n",
    "-------------\n",
    "\\* Note that you can only run the cell below once. After running it, the `folds` generator object is \"exhausted\", and you'll need to call `skf.split(mvp.X, mvp.y)` again in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Train-indices: [ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      " 34 35 36 37 38 39]\n",
      "Test-indices: [ 0  1  2  3  4  5  6 32 33]\n",
      "\n",
      "Processing fold 2\n",
      "Train-indices: [ 0  1  2  3  4  5  6 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      " 32 33 36 37 38 39]\n",
      "Test-indices: [ 7  8  9 10 11 12 13 34 35]\n",
      "\n",
      "Processing fold 3\n",
      "Train-indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 20 21 22 23 24 25 26 27 28 29 30\n",
      " 31 32 33 34 35 38 39]\n",
      "Test-indices: [14 15 16 17 18 19 36 37]\n",
      "\n",
      "Processing fold 4\n",
      "Train-indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 26 27 28 29 30\n",
      " 31 32 33 34 35 36 37 39]\n",
      "Test-indices: [20 21 22 23 24 25 38]\n",
      "\n",
      "Processing fold 5\n",
      "Train-indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 32 33 34 35 36 37 38]\n",
      "Test-indices: [26 27 28 29 30 31 39]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for fold in folds:\n",
    "    \n",
    "    print(\"Processing fold %i\" % i)\n",
    "    # Here, we unpack fold (a tuple) to get the train- and test-indices\n",
    "    train_idx = fold[0]\n",
    "    test_idx = fold[1]\n",
    "    \n",
    "    print(\"Train-indices: %s\" % train_idx)\n",
    "    print(\"Test-indices: %s\\n\" % test_idx)\n",
    "    \n",
    "    i += 1\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, StratifiedKFold determined that for the first fold, sample 1 to 9 should be used for training and sample 0 and 2 (remember, Python uses 0-based indexing!) should be used for testing.\n",
    "\n",
    "Now, we know how to access the train- and test-indices, but we haven't *actually* indexed our X and y (i.e. mvp.X and mvp.y) in the for-loop over folds. Nor have we actually fit the model on the train-set and cross-validated this to the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: in the code-cell below, complete the statements by indexing X and y to create four different objects in every fold: X_train, y_train, X_test, y_test. Also, we created a new classifier-object (clf) for you based on a different model: scikit-learn's `LogisticRegression` to show you that *every* model in scikit-learn works the same. Use this classifier to fit on the train-set and predict the test-set in every fold. Then, calculate the (cross-validated) accuracy in every fold. Keep track of the accuracies across folds, and after the loop over folds, calculate the average accuracy across folds.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-98-363bf77f3765>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-363bf77f3765>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    X_train =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf now is a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "# run split() again to generate folds\n",
    "folds = skf.split(X, y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    # This is a slightly more concise way to unpack tuples\n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # Complete these statements by indexing X and y with train_idx and test_idx\n",
    "    X_train = \n",
    "    y_train = \n",
    "    \n",
    "    X_test = \n",
    "    y_test = \n",
    "    \n",
    "    # ToDo: call fit (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE AMSWER\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf now is a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "# run split() again to generate folds\n",
    "folds = skf.split(X, y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    # This is a slightly more concise way to unpack tuples\n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # Complete these statements by indexing X and y with train_idx and test_idx\n",
    "    X_train = X[train_idx,:]\n",
    "    y_train = y[train_idx]\n",
    "    \n",
    "    X_test = X[test_idx,:]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    # ToDo: call fit (on train) and predict (on test)\n",
    "    model = clf.fit(X=X_train, y=y_train)\n",
    "    y_hat = model.predict(X=X_test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "    accuracy.append(np.sum(y_hat==y_test)/len(y_test))\n",
    "\n",
    "# ToDo: calculate average accuracy\n",
    "print('Mean accuracy: %.3f' % np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-fold cross-validation instead of hold-out cross-validation allowed us to make our analysis a little more efficient (by reusing samples). Another method that (usually) improves performance in decoding analyses is feature selection/extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Feature selection/extraction\n",
    "As discussed before, a small sample/feature-ratio often results in overfitting (optimistic performance estimates on train-set). It follows then that reducing the amount of features often has a beneficial effect on cross-validated performance estimates.\n",
    "\n",
    "Feature reduction can be achieved in two principled ways:\n",
    "\n",
    "* feature extraction: transform your features into a set of lower-dimensional components;\n",
    "* feature selection: select a subset of features\n",
    "\n",
    "Examples of feature extraction are PCA (i.e. transform voxels to orthogonal components) and averaging features within brain regions from an atlas. Because feature extraction often does not use any information from the labels (`y`), this step does not need to be cross-validated (but you *can* cross-validate, e.g., PCA by estimating the components on the train-set only). \n",
    "\n",
    "Examples of feature selection are ROI-analysis (i.e. restricting your patterns to a specific ROI in the brain) and \"univariate feature selection\" (UFS). This latter method is an often-used data-driven method to select features based upon their univariate difference. \n",
    "\n",
    "Fortunately, scikit-learn has a bunch of feature selection/extraction objects for us to use. These objects (\"transformers\" in scikit-learn lingo) work similarly to estimators: they also have a `fit(X, y)` method, in which for example the univariate differences (in UFS) or PCA-components (in PCA-driven feature extraction) are computed. Then, instead of having a `predict(X)` method, transformers have a `transform(X)` method.\n",
    "\n",
    "### 2.5.1. (Univariate) feature selection\n",
    "As an example of feature selection, let's take a closer look at how UFS can be implemented using scikit-learn. Basically, it follows the following structure:\n",
    "\n",
    "`transformer = Selector(score_func=ufs_function, other_args)`\n",
    "\n",
    "Here, the `score_func` parameter takes a function that implements a univariate statistical test (like an [f-test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) or a [chi-square test](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.chi2)). The `Selector` class determines how to select the subset of features based on the result of the `score_func`. \n",
    "\n",
    "For example, the [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) selector selects the best `K` features based on their scores from the statistical test. For example, the following transformer would select the best 100 features based on their F-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# f_classif is a scikit-learn specific implementation of the F-test\n",
    "select100best = SelectKBest(score_func=f_classif, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a selector is `SelectFwe` which selects only features with statistics-values corresponding to p-values lower than a set alpha-level. For example, the following transformer would only select features with p-values from a chi-square test lower than 0.01: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "selectfwe_transformer = SelectFwe(score_func=chi2, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: The functions `f_classif` and `chi2` are functions to calculate statistics for when the the DV is categorical (i.e. in classification scenarios). Can you think of a function that does the same when your DV is continuous (i.e. in regression scenarios)? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does this work in practice? We'll show you an (not cross-validated!) example using the select100best transformer initialized earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [     0      1      2 ..., 236797 236798 236799] are constant.\n",
      "  UserWarning)\n",
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236800\n"
     ]
    }
   ],
   "source": [
    "# Fit the transformer ...\n",
    "select100best.fit(X, y)\n",
    "\n",
    "# ... which calculates the following attributes (.scores_ and .pvalues_)\n",
    "# Let's check them out\n",
    "scores, pvalues = select100best.scores_, select100best.pvalues_\n",
    "\n",
    "# As you can see, each voxel gets its own score (in this case: an F-score)\n",
    "print(scores.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize these scores in brain space (it's a coronal slice sideways ...): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAAD7CAYAAACxDNw9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnd1vXdeZ3n+HLCmaNClZDCNBisaJbI3cdAxPBi46CMaY\noje9aG96MTdz2Zv+Df0Lej1Fr4qig/auQFF0munHtEWmQQcugqYBgng8ce24UmQTkmlyKNHiHJLg\nYS/WfvQ+++U+m6QkkofJeoCD87X32h/rWe/3WntwcHBARcU4TJ33CVRMNipBKnpRCVLRi0qQil5U\nglT0ohKkoheVIBW9qASp6EUlSEUvKkEqelEJUtGLSpCKXlSCVPSiEqSiF5UgFb2oBKnoRSVIRS8q\nQSp6UQlS0YtKkIpeVIJU9KISpKIXlSAVvagEqehFJUhFLypBKnpRCVLRi0qQil5UglT0ohKkoheV\nIBW9qASp6EUlSEUvKkEqelEJUtGLSpCKXlSCVPSiEqSiF5UgFb2oBKnoRSVIRS8qQSp6UQlS0YtK\nkIpeVIJU9KISpKIXlSAVvagEqehFJUhFLypBKnpRCVLRi0qQil5UglT0ohKkoheVIBW9qASp6EUl\nSEUv/tp5n8AkYGEwOACYBvaP2HYamAMuNe+zwEzaZmgvgBHwxcHB4GWd71miSpCE6efYx2/ivr2g\nkOMo0k0yKkE6cBRJpmybqfTuEDn2XtJ5nQd+qVTMe4PBwTSls7xzngJbFJGv3/dpj+5MiqPUzXTH\nPjRtCk6a64PBwRVgEVgBrgM3mvMbAn8woSrol4ogGqneMfr8PKqjCy49pu2VyTRNIcs0xUaZs9dM\n0472mWQVdGEI8puDwcE8ZQROA9sUyeCSQDde7zS/e4div78IdIw+HS0yybDVa9bORef/DweDgwfA\nA+ChtbF5zpLlwhBkirixGpUzhBoZEt7EUdJiHDnU2aMx/6vtOWC+OR8nyMi2kfQQSZxQXUbtsLmO\nPWtnEgzEC0MQ3eyZ5vMuQYRdijSZa37LHeeE6Ov845zDDIUcSwRB1aYfR/85OTJxR81LxNglCPOy\nVOKLYiIJ8vXB4EBkkEieb/7LxqFu8ND+zx3ihmkXXCLJRpildNhe8/9i8xI55gmijoBNiiGcPZZs\nr2TsEwTf6zjHK4PBgQj45BzUzUQSRFDnQ+nwOeIm7lBu6lPCO3GiuP5X0Mo7wCXJHHCZQoA3gNeB\nW5RO36SQ5beA71DI8QD4nGIH0bT5KfARbVWh/+g4ts51G3gMbBAEc2Kft5qZSIL4KHIvxG/ykHJD\nt9Pv0ueSCBCd0aUKpilR0SsU1/M7wHsUQnwOfEaRFr/9LeD3gTm4/n14909LpwrvN+dzr2nfvZTd\n9F3nIBd8E1izc3Rb6EVU4svAxBBkqQl3a+T7yJPBt0tICA9nOzncq9mzttzwkx2j7ysUifE6hRjv\nTQHfhV//FG6tNtu+DXy72fkeDD6G5S2e6Z7v/kVIhDWKJ7Jhxx6m4+pcpZq26FeDfo/g7NTNxBBE\nhtm4EbNLufm6wU8paiYHuvJNztJogcMeyC3gLqX/f5Pmw93y3yu7lN67D/xx0+CHwCoR9boDy9Pw\n9z8oUujHzUuEeUpIgx37fY4gx25zLl2BtvOUIhNDEBg/elytCNuEgQgRPZXEcKLIA5Kxu0zpHEmr\nW8BbFOlx/RohLaSv7jWvj8v3p6PS6V+Hwoi3gavwyg147yNYvF8kw33CJtkmpIg8rjnCjhpn3Pq1\nnQfOlSB3BoMDjbBxkGSRuvAb1xUA81hEdi1FEsUx5LXcAG5TOMFd4M3mhy2KBSkjptFnC5QXt5vX\n1yhs3QCuwNX7hYRXm13cVpGBmmMfXZA94uQQ6ReatAKcrro5V4LIK+lSLd65IolcQnX+nP2v/fXu\nLqs6Qftp3/nmXTbI4C5wp3ndpogBGRK+g3L8VyjuzxJF5QzLPiOK9rlO4Zi73ZfsPD0oNkM7N5RV\nblcoX/fpNHHuBBl2/O4uatcN82DYiEKaHdsXIqA1RxHvrm7k4SwQHfk6FOlxl0KOb1LIsdbseKN5\nXSUMmRFF/MnfHQLr5ViLwLVmdwXwFF+RAa3d8mCg43MXJBFPE2dOkL8zGBxIxOoGyB11SdAV8PL8\nit9suZXuobiLu2/7z1MG/krzvkjhwuAd4HeanX8I/BkluPFps+Obzf/fmoLdUds2+RD4SXn/4qsi\nTNYoAsi9LA+zizQ5LnOc+hFJQt2L08SZE2TLPneJ19zZ4ww0iWsPpnlSTFFQ6XlJngWKfSCCXKEI\nDN4BfpdCjD8BfgD7O0VALH0N+AfAt74N/D2Y/QBm/xxmflGiZn8G/BTWvyx27D3Czd2yYwsuwfw8\nuzrbSZQHylmE48+cINsc1psa8R676Eql639JFUkQSQ2ZCQuEa+mR1TmKxLhKW4IsXKbYHW8D/xX4\n7/DPRoWAC8Dvfgk3Afi7lGjZfwQew84vioT5Afx8VLwWZWQlRZRx7rpeqVh5MT5IuoxxXUcmxs3B\n4GAO+PkpGKtnThD5+/kGqKO7QtO+nSfrZACq0z2Pcs32k50ij0Xmwxalc/78MfyNf0np0X8L3x8V\nSSAibQA3PwL4V8An8MX34D8B/wH4I/j3oyDEGmG6bBJ2RleQTAk6TwP4wHA16ffNJYnbVKeBMyfI\nkLbbKlUi+CjaTfuKANp/SHEgrlIMTe8ET67tWFvPXFSiyuyHwMb/g9v/tAS43qcIhr2mzU0odsb/\n2YDF78G/Bv4Qfrpatn2fIIPaVI5I5yOPSwEzJ4le3uFSldPAE9temEnbn5a6OXWCeAjdPQ+v5eyK\nY0g3Q9soE2TQTlFUxS3a6fdrFNJcoXSaDEbhCRHmWAV+SumQVYqK2CBU0iaUTNz3m8b/EP5dQ44P\niSRdrj9VFlgxFyeDEo6543Nmmea+7aT/pUolDXeBtweDA+WnXlYV/akTZN4+a/TkFLhb5C5R/P8c\nE1mg3EARYYW4oTS/36AQ5D4h+hXvekohx2azvQisDO6QQqKnzb5/eR9e+8/AHvxitUiYVSJMLmLK\neFbnySAGWG/OQWoH2tnbXKKgAKKknxPnFhGnu085n3scnc85KU6dIIvN+5AYMV5ZlW+O6jC8fNCN\nVY2+y5Sbf5NChmsEQRQ+F0EU61onpInIsUnbKHQpsN28NoAPgL/1p6GSPqZ4KpsczgktEPbLzeY8\noHRg9sjcCHfk3IzuhQbGLUpq4LsUJ2qDtnR6WTh1guQb4DEMBSRnOHxRIpK28boO/b5IIcpK8xK5\nFMlcAWZfhcWvwm7ZokiExxQJ8ZhuES/9T3PMB83v25TRukbkV7rUhNTTNcoo9zrZOYKc3qneRo6y\nKqi3QiHfNwmpOdOxz4Jlfp++gLo5dYJI77slLlJodtocbZHqyBZ614hSLcceISX2gdkpYKkQZJGQ\nMO5aZshVFvm031MKSSRR+hKLLoUWmnNbbM7XC45kgObEo9TVLpFglJ11hxKzm22u8wcU+2nNjv0y\nE3tnShAZlR7Qkq7uGkUech9XHKybd50yItcpN30Ez0rFrqyGsQiRcu8iiNzmZSKQpnC9UvObHO4E\n/65rkSS7QZEkavcyIY1EppHtkztaKus28C7wNykk+zHwvwkX+zQyvqdOkFx6R/rurp5uiqKjGlXa\ndpeQQttEJdZ685Jt8Zii7y9tw8pflCi4jEoZql7KKBU2RenIWxTbYb757QlBqi0iQprLGP3atO0D\nygh/SBDsUdOGXGGpqS5y6P7sNPs84nAwToayqtZyGP5F1M2pEyRLBCWXFOX0uIBuuNeQupWv7XcJ\n72OOiIhuUzpirdnmfnNMpVRWm/2kynLibo5Sk3qXkryTGyzPQ52ugJjXw/o1qlM3KW7wQ0KN7hPE\ndjtmXL0stOMgsofkXbk0VTLTDeYXNVhPnSBuM2iUehBsZJ/37D+fHZ+hm6yw/SKR13hMkSb3iBG7\nQRAj2zCyCxRYe50oKPuoac87UhUA63STQ9ckgjykHU1V3imXS+a0vntVfh0Pmmt1+03b+1SLUWrn\neXGmEgTi5KV3PVrqejjvM65tiXGXMjJCpRYUdPPRlRNemnqwSrnpGxRDUmJc5PWRLg/Lg4AeCof2\ntI2rFBtE5+1ejEdSIaoIXNX6QFq04ysyLHgCc+IliCMTousi5I76SOqrlxBBtmiHnT3p5UZuzo5q\n1G1TCPUpZdQvcNjbUQ5InzWCVeOqMkaRdER4RCsU9XW7aeMRYZeoqs6jp2vNNlKlTpJ9wj6SO+7F\n3l7/8qI4M4K46NNF6nfPy+QkXh6VOeGlYNYq4RHJpfRRneMtOYKrUSz7ZUS7Ks3jDdhvsn98YrbU\n0JA2Od4BfqNp+x7FRlqnqLEtO/dpClHdNsPOyyPSHq+BINrLys2cGUFyLYNw3MgiHJ7r4uIXDksi\nT3qtEJXxbouoE9yT8pvr7reXFOTs8ZRt78m6HcLmWKOQYp+2dLjavG4Q+SMZsVJzyg/JHlHx8yM7\nP5H6atOG8CJliWeqYsapCqErKeeQvp2iPastt+ejS0ktiW6NWLmuUiHq2FxrkgNfUDrgJu0aFmxb\nTWtwb0xeh6D5MIrjXKUEwe4Cr3ytNHCwHYnEDyhSRYQR+bQSgO6dAocuRV5EmpxLTeo4d07wWWgu\neRR99XS/JE42DgWF5FcI0mjEq9rcJ057llnnmiXTApGAe0Jb+vgUB0kSGZyaPYf9b1NreHuKUrT0\nZvlzsAc3V2HxgyDAFKEKRTJJMUlLqT3H80ZYz9xI9Xehy/5w+GjwQJo6Tx3uBqq8ErnBUMh12dpt\naowP3bxpe0mlyVO5TKTvFRfJYt5rUeUaTxNk0XXsEgX0b1+miI+3KLpGvvkmLE3B1VEhkuIyQzu+\nvKTF5hqzoS5J6Vnk4+JMjVR/F7rIkYM/3lES514vIlvD1ZMnruR5eH2Fsrtd5+SZZp2f7I0rhJrz\n5N/Q2vK5LpqcLY9GsRud2xxNTezvUDJwtyn65rOmsUYkrGxEdZvXuOjeKJkndSopLPtHwbmT4tQJ\nkkO7HvbNXoo6WGpEotMzoRDFM/u2n1fG58ncG5RgmL5LAo2btJTrVvx81YZPoVSoO1ekSxUqEqpR\n71nseWDpVYrkWCECHLvW+NMgvNShH+9ZOwQx5OFJyigFcVKc+9TLrmScklMythSel5h8Yr858uiF\nsFOGtANKikx6EjHnMJyArtbkrj+hbdhm1ekRY33fJyKhit4+s1IhhvqnwM+Aj+Dznci/yEhVLsel\n6DztVQ9kA8lzyyWcx8G5EMQNzy7PZZHiJXyDkCQjwvVTwXKXS+xxA0kL5Why8VHO6Or3HNmUJND7\nyPb1Gls3rgVtu2vt6jzkcTybp6mhvkapSPoQ7u2UmImSc3q5nSPje4FQaUotbNE2ok+KMydIJod+\n85mN36TkRL5J2xsQulLtXaFwCNUzIlQTHPZOMrqIl1WXOn+cyy64hFFmVqN6C4ox9CWRml4FPoFf\n7MQ8G02p8KkUPknMjXOPwwhSacum4gHWj8junosE8ZEp+0LVV9cp6fZbzfdHxI30FYV85HscQiN8\nxl7uAnel13NspqvD/Td1UA6w5QSZR3G9HdW7TlG4sPsZzH5IFK0+hL/cKFrmYyIbrcTfiLYdI9U5\ntJckngbf8+LMCZKNUn1fJCqmFFFcIYJBiiB6mZ/gdSLq9LwuqarK9+muJpMdMi5WkCOuw/TfuOt0\nkuaA3B5FKnwKvPUhz9zav3oMnxDE+Ihijni+SSWNsmcggmciSC6dfJ6I6pkTxNf9vNksYeAXrLnR\nSsR5UQ20Sw1l7Pu8F5FO4fAl+91HlhMkp8i74Easqw/PDufRKsno5Y46tozHe5SqsCs/iakYGxTJ\n8TFFtXjJgO6T55Y8e6uwvQadH/t5cK5ejIe1ZWTJc5ELqboPVwdeByEfX26f2tQc3KtEx6tTugJy\n4+yRHKH10eiBP5cWwizhoGhlxIXmfO9RiHAf+BEx+qVOZZRKeg5p54HgcEHVFLHE93Wisv7CEuSB\nSZPfa1ZSlvTYIUZUdtE8PiIJo0nh6ihJIxUzqyQvB9SOC/e23PWF6CRP18sOUh2qqtFXKB2/3uy3\n2my/Sng624Q54olF3Z8rRJDObSol6t6iFD3dAJY1SQeey4059ziIcJVy0Z8TFe5D2uV+Plo8Meb1\npfpf637cJoqH5CZ3JbG67l3Oy4gYIqe71CLMMG3rNsd1SjR9hZCQkgK6bifJfmpbpHiY7pGfzyOK\ndJpp2mIHFnfG52JysVHGRBFkRBlJbszJn/fQOYRU0E12SP9eI2pLNRVC+hvaRTbQTRInk8gxRzEM\ndVwvNlY8RGrH1YCWP1sngqRyXR/S7kSvk9m3Np8Q6tSNURnhqn8VlKwc59b/esdvjokhyD8xdfOP\nmkVmRABJEJceMk7do8mekTpzh7Yt4bEDvZTNPSpxCEEUb0+iXnNZdJ46VyUVF2lXzyuXk6dSuLfn\n57KXXlItyi+5hJStdpXxEuTCEMQh6eElf/ptXIGPj5oRZZR+TEghVXkpYaWEm3s0I0I6uIeUjcE5\ne5cb6XN64XDHakbe+81xHqU2VPUu6eBGr0oUoL1WrGeQPWq8Ttg/8xxWgyfBRBLEq9rdr5dUyYEp\nub5+s0QEjU55Q5I2Mla9I/cp5Fhq3rcIgmDvasOnS0A72uuSTP/9vHmfTm3Ig/MAnmdplYiT6w9t\nFSbIfVbbcnO9pBPb/ziYSII8pi09nCB5qkCOTegGa+knlenJG5BRn8V2H/bTZyfZIsWr2CXKB1zF\nCdsUqfGYdimkG9kztq/aVTBMpQJu14yrh4Ew8B8QRUUi8jh104WJJMjnHNazLjU8PA6HRa06UAac\ntvfiY91oRWHVvtzrWdrJMKxdLzGYpxQRbRLGr1TeJftN17JF2xPSjMCnhGu+RCyqeIUgjRKV63YM\nryZTmYSudUhkfjWPBrtPx8FEEmSV9kX4xWSpkUmhkSEV5cU+ru+dRE7Cke2nY8hD0Hc3gDXSteBM\nth1ELl+szgN6W4RXs9y8blHc4TuE+z+kBNUUXPMaGK+7zdMfJNWkxnQ/L7QEyUkvjxTmYtxssGp/\nJ5c/qUq/ecwhby/iuc7On2dsOx3fz9ur3X36Qp6N58+J2be2JZ1km0wTEsnbniFmBvokeJFKy1tI\nnen+HBcTSZBL9lmdqpsiD8PjDgo65QhpV8oeuj0h79xciuD7egdCeEm+IL+3o+o0P5530Mi+S8V5\ndLjLu8lJQKm5JWLNE8/s7tvLDeTjYCIJ4qLZ9b3qLiXyZbVrJOeKqXFpe5FCcYuuIuocB9ExvIOU\nAFSG2FdQmqZ9Xk6OnIl24sq13raXzts9HwhbRmpOxnImiGyxnLc5DiaSILL+c8rcl7vUDddnjQ4Z\nfy5qvU4T2nGDaftNGEcsCENTyy6IwC7KfSKVSw5Xg9rPZ/3vW9tavAa7/i4iT3Vs48Rw1eXXeaEJ\n4iLeDT53/XTxU8Qk6Ox2upRRTMU7X+QTMom6bqLKHUW8bNdkQ9Hd9Hzc3GkiyB5RXbdv1zwuXJ49\nOieI4Mc4CSaSIN5pLhWkYjSyVEYoVxYiHe6heln0HoF0AkL7xvnn3Il7xJRJ7xifxedljk4St28k\nPbwNr5JXWx69HUcQwQ1mEcSJcVJywIQSJKsYiKJlTU7Wbz5XFSJA5rERT+hlImQjtQ99uRrFGdx4\n1bvHYWRUKgDmI17FxiKKosGyNRTs04AQGVQzMkOUtXZJywtRUXYcdBW4SFUo6ylI54pUIoxDxmge\nQe4dnATe2UJesdFTA9n9dVWo89uzz3ppYTrtk4OEUqO6XpEtL4p3nATkOEwkQf5Hk9n9283zcyGI\noCIZwY1Y3RxfAkGZ2txJ0HZd3WA9aqS5J6PvLhW8yMk7Vd6YgnWLhN2ijlab/jRPX9ICwh7R9Ypg\nyiTnckRd0y8NQYQnxM28RKyHmuMK/kChPnhHwmEJkonh3ofDiaWIqp5Bc5UoTBqlbX1/2Qtuo3jM\nws9BdarybFYoknSvOY5qcLVmrJ7Jp1UfPQp8Ukw0QeRGiiQeB9Go0cqG62lflwjCFIfrXqWrp9N2\nOr67pthnif4ZCnl9fQ/Vh+QlKhw5HuMJSTcsJTXXm+MoFK/cjLy3BQpB7lCq6BaJuTRZ5Y47py5M\nNEF041wsq+pdBci6iRL72fV0dEVJx5UVQrtSKyPngHyZCXW8FruVG55dX89W+/cuqSOIkDnlIC/v\nOqUedZlIOEK3PXQcTDRB5IFAuaCrhEh319F/U3Gy4Hp439rzcL0jG6Aip0sOd3n3aEuheYok0fTR\nB5Ts9GOig1Sz4eSQpBAZXX359IUhsTqRpIfbVJcpMxJvNNs8IIqfuyTiUZhogmTX9AYRP3CpoU6U\nkeYPDszqQ6Lc4xGCu5Dyinxd12wbKNSuc1S011XYJTsXCHEvcnjuxe0d5Z18OoiuW0VQitxKMo2I\nR6wtvwo3vooSxz0iBH/hCZLnj0KMMBl/OVII/UEhjUhPk8tQ9H1zIXNX+zlXs0fM4dkgnjShelit\nBSN323MiXgYgSLXJ5lKeZYZQWar292t9QpEuHwPDr4r0UCmBvLuTZnQnkiAZsjNUkLtJu9TfX/7s\nluxt+HNcPMiWJUMOXevd8ygu1j0/s0oQ5LVLsNEs3fCQWEjGyeB5IS8jmCc8E5Fkn0IMXxg4q4wP\nCY/vU8o6NHrAwbBj+6NwIQiike5LS2oykpNhRFsqCDlA5UU42c2d5rBq8xC2VJw/NldLg69RCPgu\n8No88G24/aMyr/YKZXQr/O/t6xzzuS4Rz26+TEiMvFa8V8P9rDkXlShqEGgtk5NUk8GEEqQrkipb\nQR2bHwQkq13Fwn7j1KEixzKhsrwS3cWu6/WukLXnSCA6bpoyim9tw1s/KuJeT3fPgTlvEw7bHVoL\nbYdCLs8ie7W/w+cS5aLv58GFIQgESVRprqSWpIlu6Ayhc5UZdV2+0vy3SbtcIB9H9RxdgbTp9IKo\nnP9J0+59igejeAW059PkY3qFuyfrnhKFQGvEEp5Sb1JLSvcrfpTrUPzcL3Qc5KjJxpoWsUfUcV4m\nLHrVZ9Bs50E2kcRrVd1oFaYIeyZLEA/Jq2P9iZfzzT73iU7y1Qe8kNklkqtBlyC24OGzVZmzjeXh\ndV8Kw9VlV+3LUZhIglwmOiIbkHInleqXqtBToPaIZSGhPdNtaNvJ6+gy3HRjPRzugTgZza4qsvvo\nHa4OfEzbAPaYywztcL0Cg5IAsiV8hqEkmlfqq12dy0zaVvfuQkuQvFKwbAGf2Kxg05Aictdo32zV\nZbgbu0PoZ63l5Wlxdb7bOzq2d6bncPZtW895+HlIEjzicCdOE3W2IogW6VW7XveaXW2RT16d7pcf\nQ9fmuHBu7h8MBgcPiOeheDzCy+1ypZcIo2kHPv9D+t7jD4JC29C+2TpGFuG+jUsYqSC3U+YoUvA6\nbVtinrY6EXHUuVKBV4lcTs7V6HynOt67MJ3ehQsnQWRQqkNlgQs5ueXxCkkTn5eSXVSPZkJ0pJNx\nipBO20Sn5biIE8hjGEos3gZ+m/J0Bw+lS/rMEarE2/SCbH8+HhSXtys+48HBrJLHkeMkmBiC+LxZ\nhYV1c7J9oI7RaN5pXl5E7BKgS+fK+7lKu3ZEHscW7RyIRztFOJccIoeeWvUu8MYN2F+NJR5E3BnK\n7P63iBpXeWRy0SUVFcibt/2l4rw8IKsM3YMZunEhknX/uAmpS2ooEKXqsc3xu7bcuhyqzgTpghf7\n5OfK+NTIPBPPo7Y+ev3BQXeAN14F3oXpD+HaJyWq6asCLFOIJMNZBBXRfbUA2TKSgv7kqSxl8+fn\nKTN0nCtBJHa7Aj5dEVFBnagyPDj5KsKKgrqbq5jDNLEiNkRHuF2Qq8T1BIjbFILw7ea1Da+sweXH\nbY9CNorqTN0Q1buOpRS/BpIGkY7vxqoTyK9VOClhzpUgO7QNQ/fVs83g0GjLdZpZYowLCOl3dYof\n0z2QGdtWIXLP3jrmiSWv3oR4MuIacA8WH4eNNEuxKa4RKfinxPolCskrnT9NGLo6V1d3Ol9fQ8Tv\nnRv2JwmzwzkTxNfTECGkLrrEJUSHudHaZ5h5HMJ/V7uumiACaZp74/mariyxIIN2mWbpTblQzU45\nGqsqdtVp+MrLm7RH+rhgncdDVDvixnre33+7EBOn3Mbw+k/5++6N5HfpXvdSHD7hSi832BRg07Fk\ngywQpYP+fFw4nOb38/HiHiBEQvOYBbWha1qn2CXKUq8RKXyFzz0WpMp9Vx8eQfUSAQ+zK0H3vLbI\nuRLEAz+yyrUorkbCkMMWuka+k8ZvggepFml7I2prk8jp+OjSo9TfSOc4ZS9BATTZCTJUpyDWXmgM\nCl2HOnmDshqh1Jy8Nn+ChKSfx10El5RdUVsNAN0fnW9fvUsXzpUgCntDO1LqWdRx0UAfzd6GZ0Y9\nB6NKb/dCfBUBQc+W082+RTEo/TGmbqT6KH8I/JhirL73JxT/9ifws72Sl3lCSJBNQoJIvaiMUNfS\n5UY7ESSkpG6VB9JAkAPgxNqctEez9yE/ASkHf4T8bBiXFCLRXvrudRuKTurp3BqRXq4nbBFLcV6n\nPJJklpK292Mpx+Pr1H5GIdIMsLddakEeEI/z0HIOe0RVmNqSm+82kdseLqHk8mpilbu6VyiDYaZp\nVxLpeXHmBLk1GBzk5SuFnITKIfD8WejydvIKQNnw1LPecntaS2xIPHlChUmyI2QreUGxqsnWiYjs\nLWK1aJ+U7eWTOaeTvTj/Xcdbbs7F1w7x+3OZQqJ1u8aTZHAd5/K8GOnJo/IH0LbAPajl8LS5RvVl\nihvpT45Qwmy9ee0Sxp0CY24UinhzlM6eocQ4tI3E/QwhEVRjsktROVoO3GtU5M76CoyqepdKkK3g\nUdNtuhe/032SPbPabKM24flJci4EyS7nUQaTSOIBMjjsOrqxpqjmTQpRNCPvEdFBqhXR6FTHuvEK\nQZDrdvzsSvrj0j4lntJwnUKqa0SwbbP5b42iej5r3pVAdMNckkRBPSUjdX654zcIgvk0Cr9PJ8G5\nPS8GDqsKb+nxAAAKCElEQVSGLqLkTKmntXNwzQNGmiqwQqgYxTQ8de52xCLh3SjBpmimVNTl5vPg\nKu3aRp3sHvyXUTv/8jrwa80CYgd7Ub+61pyfbBitlLgVTT2TIlJLknK585VA9LVLuiLUJ8WZE8Rr\nOOHoAJTguRLNe+lyf7NKUo2HxLtLB5cQIhNEYY6eLThDmYwkIg1uUHpdse094lEOC/Ddfw4/JDp0\nBYquG8JgCK9tw5XtIt303BslCvWUB0/xKzYk4irApuLn7N35Z79HJ42iwjkRJLtyXZ2d4eHvXBgz\nDl6l7o8185Hl9RvXiFyM1uf4lJjTewOYnWk+3LWLGVJSs38duAVL75enZWvh21deLb8/69U9GOyX\nNt/5JDr1cyJe45FVBdBUna4B4tVlfs1d35+HHHBOBMkX1kWObNVn5CIex3T6H9oupCe2cpzAjy9b\nYYYojF7Zg+VV4hm3OuAiJbr2G8DrsPDBmE7xC5uCwaXyuI7LtKdgSProPHT+UrPelF+3Q4G9ByeM\nfTjOhSAaIRL/cDhX0BXSVoe6JzRDmxCeus/5DJrtl2jXpG4SZX8+DcJT8eLCJvD6Ktxaha9PEfMo\nrlEqhJqTl3czBHa/gtlVQiTYBJUvRkVSSV3oOJ6zOaqIm47r1W/jBthxcS4EcVXRldbP4Wx1ro+i\nOUqHelGMV5lnkqidWcpofUJMU4AgmuawQNSwylXU811ksL4+gjtfltfCXVqLl8lW2qOQ7PrncPBV\n+ylR+k9zXaaIBXF1zoq+dsV/cmAQ2oPFM7zPizMnyPdM3N0ZDA7UcV5wnMPvGbnkD9rzTbxAWXN5\noe1C5pV4ZHNo+SbB3cxNa0dF0Xp61IIeBXUf2CrnMt9sew/Y/6r91ImhvecnR+kYZrIcqj+h47sP\nhhdRK45zDbX747A8viBJoZHcFSpWQs+NTa9G10TmLIF8ArPiJTpeLg6Gw3aMFyuL0PM6yVXK3Me1\nMGz1nJgNwkORsSw1m2tQ4HCGVxiN+ex4UbXiOFeCuCjVjfa1yVWM3BUFlBRRZ/qUhH1i4pREt/aB\nttRZJELhyn5O2bsTT4VDu0T9qEL6zwjSPCBZdstTCkEoP7NGuw5EKwNpqodUggiybvchlzxkvExi\nCOdKkPdNDP7WYHAg+8TjTjmi6KPf60X9f70UGdWTEJTskiGoQh2Fyddpq64FO4bXl6iNZYrHO3iT\nEjJd4JkVrdn5Q2I9MZFQ0sMlo2pAJE2eEKpX1+h1Hhkj4IuXpFYcE1PVvkQ7yurJqq66U3W450/0\n3DdFE+U2aukqJd++QUQjlTTT6PYne2vSky9I45OhVoCbM5SYyE1aa2cvEM/Ild3hRTz5wYSKv/g9\n0HXS/CZ7JRdkP2+M4ziYGIJoaccsPfziPZElm0XzVuQyu5egObIzFBF+g6glnr4Ef7UTE6L15En3\nRBesff0OIVVWoLDtDrEEEOWAkiCelMvk9zm00PZEpG5lfEuK5WIpgKenIDmEiSGIP8Wya2EUIae/\np23bXFScX9M01eHNqiyvbMJss0zTN4hK9y7xL0kwImImN6CIpWXas54W2h2qaZ5eBJ1rP6CdwRVp\nlJxT7EhS08Pwp4mJIYjErXdQXwheVv8UUXHuz9fVvqPUxgIUfdNYhdOzsDADC41hsexZuqcRu5CN\nIjUzR/NUa02Rk8XdFIrkhx0qMirSdHWuBwBFaM2huRxNP1OP7pGdFiaGIP/NxOTNJj6SazCz3vVJ\nU13Pa9m3/3yVnWfhWFXcaJHTW0TwZQd4CIOHsPQIljbh8iimasxAhNs9vWy97ueQZ8G5sekq0mM5\nslPkMu8RnhEctldOAxNDEIfXOkC7LlOjT0s8elpbN3LWflfNpjyWBWDmM/i1R4Q7skAZptdpR+u0\nRGEzve019bRH97Yp7q222wLuRf6kSwK6KumCIqhOcK3PJtUyTTHsF8a08bIwkQTJ6X8nikjymJhg\n7VFYeR1ubD4hnv6owf7OHtzZgFnFtpXvX7RGRsSKLXpwrUfbtFKuqn2a0rD9z9prjwhyYbNtlXMo\nslsUDdZ5z6WXB9ZOCxNJkPVG3VzpWA4Twl7Z5XBFmG6kRLoGPc33j5ttFDp/6z4suw5SomWWEhW9\nRwnJrlAkDIRPDJHRU0+uhbssVaRz8+TkdPrs2E//qy23r/7nKXoujokkiNBXI+JRTAmArtlzitJq\n9K4BP6W4tB9R+vzal3DrS7jxv4qg0NQB9fsMzTyZN4nSeFUcSweIceuRUFS8RNFaqUIRINfXegg/\nT/ZyF7lr7fXTwkQTxEVxHnE++16V6xplbr9oe01+VvW5ly/6o85EDo+i3gbeeNhsdJ14coCHbps5\nk093IlXgUdcFioDyCvRcuJ3rYD2zK/tD5ZJnhYkmiAeAFkzdSHT7zVc4OgfXMtSnvs0GUQnulepq\nexO4PYL3/oiiaj6C/9uIt6XPiguqObWqJJ+mlIisUziluSu+DkofdI2qrb0M/IszUiuOiSbIOCjS\neIUY8a7fx0GSxY07SSaJbtknsj/nCLfyZ1/C0pdRkQ6Hny03RaO2KNF3eU9eMO02kfYZh3lKEO+N\nI67ttHBhCaIsqHeOoqoZHnCbTttDhD2ept/VcasU4/aPabvVnsDzNVjfpYRUvAxVdvCjdI59JJGa\nugV8p+P/s8CFIci4fMPv2WPLuiCpoTiJT0rKuRB3GT1Qp9HvhiKEHTFM31UlpuW6RZxVwu5Q+10u\nLoQq/DfnoFYcF4Yg43CcyUCKSmZ30u0VJf+yDTOy966aFL3LmblEsTs+J+JtmsSlfJ4XII1rT+GW\n88aFJ8hJRtiSGboe8u56YqVLmOPYNlI7O0SG2FXPMt32kiTc5+csKcbheWbjXWioMyXe5bF4XuN5\n8xu5NCFP0OrbflJx4SXISdAV4vYgWs6R9OVLjjqOVJVHPx2usk47XP4i+JUiiBu6y4PBQVc082XA\ns7b6LrzoRKazxq+cihFctXThuDemSzXlUH/X+0XBr5QEcchYPAtcBFtjHAYHB50J04oK4FdYxVQc\nD5UgFb2oBKnoRSVIRS8qQSp6UQlS0YtKkIpeVIJU9KISpKIXlSAVvagEqehFJUhFLypBKnpRCVLR\ni0qQil5UglT0ohKkoheVIBW9qASp6EUlSEUvKkEqelEJUtGLSpCKXlSCVPSiEqSiF5UgFb2oBKno\nRSVIRS8qQSp6UQlS0YtKkIpeVIJU9KISpKIXlSAVvagEqehFJUhFLypBKnpRCVLRi0qQil5UglT0\nohKkoheVIBW9qASp6EUlSEUvKkEqelEJUtGLSpCKXlSCVPSiEqSiF5UgFb2oBKnoRSVIRS8qQSp6\nUQlS0YtKkIpeVIJU9KISpKIXlSAVvagEqehFJUhFLypBKnpRCVLRi0qQil5UglT0ohKkoheVIBW9\nqASp6EUlSEUvKkEqevH/AZOUOO/rgFBjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1150d2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scores_3d = scores.reshape((80, 80, 37))\n",
    "plt.imshow(scores_3d[:, 55, :], origin='lower', cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in the image above, brighter (more yellow) colors represent voxels with higher scores on the univariate test. If we subsequently apply (or \"transform\" in scikit-learn lingo) our X-matrix using, for example, the `select100best` selector, we'll select only the 100 \"most yellow\" (i.e. highest F-scores) voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "**ToThink**: Given the image above, what is the major difference between data driven feature selection (like UFS) and ROI-based feature selection (e.g. only look at patterns in the amygdala) in terms of the spatial scale of patterns you'll select? Try to think of an example of UFS over ROI-based feature selection and vice versa.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practically, after we've fit the transformer, we can call the `transform(X)` method that will actually select the subset according to the selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mvp.X before transform: (40, 236800) ...\n",
      "... and shape of mvp.X after transform: (40, 100).\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of mvp.X before transform: %r ...\" % (X.shape,))\n",
    "X_after_ufs = select100best.transform(X)\n",
    "\n",
    "print(\"... and shape of mvp.X after transform: %r.\" % (X_after_ufs.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the transformer correctly selected a subset of 100 voxels from our X matrix! Now, both selectors were fit on the entire dataset, which is often course not how it should be done: because they use information from the labels (`y`), this step should be cross-validated. Thus, what you have to do is:\n",
    "\n",
    "* fit your transformer on the train-set;\n",
    "* transform your train-set;\n",
    "* transform your test-set;\n",
    "* (fit your model on the train-set;)\n",
    "* (cross-validate to the test-set;)\n",
    "\n",
    "We summarized the entire pipeline, including data partitioning (indexing), feature selection (transformation), and model fitting and their corresponding cross-validation steps in the image below:\n",
    "<img src='sklearn_transformers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Feature extraction\n",
    "There are many methods for feature extraction, like \"downsampling\" to ROI-averages (i.e. averaging voxel patterns in brain regions) and dimensionality-reduction techniques like PCA. Scikit-learn provides some of these techniques as \"transformer\" objects, which again have a `fit()` and `transform` method. In the next ToDo, you'll apply this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, we import the `PCA` class from scikit-learn. Check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Now, initialize an object from this `PCA`-class with the number of components to store set to 5, and subsequently fit in on `X` and subsequently call transform on `X` (note that you do not have to cross-validate this procedure!) and store the result in a new variable named `X_pca_transformed`. Then, check out the shape of `X_pca_transformed`: does this make sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-78d6221b888e>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-78d6221b888e>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    X_pca_transformed =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# initialize a PCA object ...\n",
    "\n",
    "# ... call fit on X ...\n",
    "\n",
    "\n",
    "# ... and call transform on X\n",
    "X_pca_transformed = \n",
    "\n",
    "# And finally check out the shape of X_pca_transformed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# initialize a PCA object ...\n",
    "pca = PCA()\n",
    "\n",
    "# ... call fit on X ...\n",
    "pca.fit(X=X)\n",
    "\n",
    "# ... and call transform on X\n",
    "X_pca_transformed = pca.transform(X)\n",
    "\n",
    "# And finally check out the shape of X_pca_transformed!\n",
    "X_pca_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put everything you learned so far together and implement a fully cross-validated pipeline with UFS *and* model fitting/prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "**ToDo**: Below, we set up a K-fold cross-validation loop and prespecified a classifier (`clf`, a logistic regression model) and a transformer (`select50best`, selecting the 50 best features based upon an F-test). Now, it's up to you to actually implement the feature selection inside the for-loop. Make sure to fit the transformer only on the train-set, but then transform *both* the train-set and the test-set. Then, fit the model on the transformed train-set and cross-validate to the transformed test-set. Calculate accuracy of the cross-validated  model for each fold, and after all folds calculate the average accuracy (across folds).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "folds = skf.split(X, y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "select50best = SelectKBest(score_func=f_classif, k=50)\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # ToDo: make X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # ToDo: call the select50best fit method (on train) and predict (on test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "\n",
    "# ToDo: calculate average accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [     0      1      2 ..., 236797 236798 236799] are constant.\n",
      "  UserWarning)\n",
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "folds = skf.split(X, y)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "select50best = SelectKBest(score_func=f_classif, k=50)\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    train_idx, test_idx = fold\n",
    "    \n",
    "    # ToDo: make X_train, X_test, y_train, y_test\n",
    "    X_train = X[train_idx,:]\n",
    "    X_test = X[test_idx,:]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    # ToDo: call the select50best fit method (on train) and predict (on test)\n",
    "    select50best.fit(X=X_train, y=y_train)\n",
    "    X_train = select50best.transform(X=X_train)\n",
    "    X_test = select50best.transform(X=X_test)\n",
    "    \n",
    "    # ToDo: calculate accuracy\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    y_test_hat = clf.predict(X=X_test)\n",
    "    accuracy.append(np.sum(y_test==y_test_hat)/len(y_test_hat))\n",
    "\n",
    "# ToDo: calculate average accuracy\n",
    "print('Mean accuracy: %.3f' % np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: advanced cross-validation using Pipelines\n",
    "(Recommended, but optional: only go through this section if you have time)\n",
    "\n",
    "As you have seen in the previous assignment, the code within the K-fold for-loop becomes quite long (and messy) when you add a feature-selection step. Suppose you want to add another feature extraction step, like performing a PCA after an initial feature selection step. The code becomes even more obscure ...\n",
    "\n",
    "Luckily, scikit-learn has an awesome solution for this: \"Pipelines\".\n",
    "\n",
    "Pipelines are somewhat more advanced functionality within scikit-learn, but we wanted to show you guys because it really \"cleans up\" your code, and additionally, it is a great safeguard for accidental overfitting (e.g. when you accidentally perform feature-selection on all your data instead of only your train-data). \n",
    "\n",
    "Anyway, how does this work? Well, a picture it worth a thousand words, so check out the image below which schematically depicts what a pipeline does: <img src='pipelinesX.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a `Pipeline`-object \"strings together\" an arbitrary number of transformers (including \"preprocessing\" transformers like `StandardScaler`\\*) and can optionally end in an estimator. Pipeline-objects have two (relevant) methods: `fit(X, y)` and `predict(X, y)` (the latter method only exists if there is an estimator in the pipeline). To use it, you only have to call `fit()` with `X_train` and `y_train` and it'll sequentially fit the transformers which will finally pass the transformed data to the estimator. Then, simply call `predict()` with `X_test` as argument and the pipeline will automatically cross-validate all fitted transformers, and eventually the estimator, on the `X_test` variable. \n",
    "\n",
    "Okay, lot's of info - let's break this down. First, let's initialize some transformers and an estimator:\n",
    "\n",
    "------------\n",
    "\\* [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) is a scikit-learn transformer that does exactly the same as the `standardize()` method you implemented earlier! Strictly speaking, you don't need to cross-validate your standardization step, but it doesn't hurt either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ufs = SelectKBest(score_func=f_classif, k=1000)\n",
    "pca = PCA(n_components=10)  # we want to reduce the features to 10 components\n",
    "svc = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to initialize a Pipeline-object, we need to give it a list of tuples, in the first entry of each tuple is a name for the step in the pipeline and the second entry of each tuple is the actual transformer/estimator object. Let's do that for our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_to_make = [('preproc', scaler),\n",
    "                    ('ufs', ufs),\n",
    "                    ('pca', pca),\n",
    "                    ('clf', svc)]\n",
    "\n",
    "my_pipe = Pipeline(pipeline_to_make)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our pipeline-object (`my_pipe`) on our data. For this example, we'll use a simple hold-out cross-validation scheme (but pipelines are equally valuable in K-fold CV schemes!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [     0      1      2 ..., 236797 236798 236799] are constant.\n",
      "  UserWarning)\n",
      "/Users/steven/anaconda/envs/python3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy on test-set: 0.850\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = X[0::2], y[0::2]\n",
    "X_test, y_test = X[1::2], y[1::2]\n",
    "\n",
    "my_pipe.fit(X_train, y_train)\n",
    "predictions = my_pipe.predict(X_test)\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(\"Cross-validated accuracy on test-set: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool stuff huh? Quite efficient, I would say!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
